# 基本概念
# 深度学习基础

## 一个简单的神经网络的搭建

前向传播、反向传播、梯度下降、参数更新


## 机器学习常用工具（框架）

### numpy
### pandas
### pytorch

## CNN大串烧

### yolov系列

特点：检测速度快

就是一个大的卷积神经网络

![image-20231205142850593](./photo/image-20231205142850593.png)

输出是一个7x7x30，也就是将一个图划分成7x7，每个格都有一个预测结果，对应一个30（5+5+20）的向量，对应这个格子检测的特征。

5+5+20：每一个小区域的预测结果。每个小格子都预测两个锚框，每个锚框由中心点坐标+宽高，还有一个参数表示到底是否框住物体的置信度，20代表训练数据中20个类别的预测概率。

损失函数：五个部分

1、纠正中心点xy

2、纠正宽高wh

3、纠正置信度C，有物体还是没物体

4、背景的时候，希望置信度为0，有物体希望是1

5、对20个分类的概率纠正

![image-20231205143102602](./photo/image-20231205143102602.png)

### RCNN

解决目标检测问题，希望有几个物体就画几个框。two-stage

1、根据某种策略，划分出两千多个小图，弄成大小一样的图片

2、AlexNet--->4096--->SVM

![image-20231205165437899](./photo/image-20231205165437899.png)



### ResNet

通常的理解是模型越复杂，深度越大效果应该越好，但是实际上确不是。目的是解决模型在变深的过程中，性能不会变差。

加入一个恒等映射F(x)+x。

![image-20231205170711667](./photo/image-20231205170711667.png)

### VGG

1、证明了深的模型在图像处理上有更好

2、采用kernal=3x3，padding=1，stride=1,使得卷积层不改变图像大小。也是个卷积神经网络。在AlexNet的基础上，按块叠加。

3、池化窗口大小为2x2，stride=2，让图片减半

4、提出块概念（VGG块），可以定制深度。

![image-20231205171505849](./photo/image-20231205171505849.png)

![image-20231205171449654](./photo/image-20231205171449654.png)

## 注意力机制

设计的是模型的结构，结果是从学习得到的。Transformer模型是纯注意力的模型。

K:关注程度

Q:条件

V:输入
